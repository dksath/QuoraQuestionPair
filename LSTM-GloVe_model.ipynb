{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Quora Questions Pairs using Global Vector (GloVe) + LSTM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "# from keras.layers.merge import Merge\n",
    "# from keras.layers.merge import Add\n",
    "# from keras.layers.merge import concatenate\n",
    "# from keras.layers import Input, Dense, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers import TimeDistributed, Lambda\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.preprocessing import sequence, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "y = data.is_duplicate.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Converting all sentences to lowercase\n",
    "    Removing all quotation marks\n",
    "    Representing all words in some numerical form\n",
    "    Removing special characters such as @ and %\n",
    "    \n",
    "    All steps above are done via Tokenizer in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = text.Tokenizer(nb_words=200000)\n",
    "\n",
    "max_len = 40\n",
    "tk.fit_on_texts(list(data.question1.values.astype(str)) + list(data.question2.values.astype(str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tk.texts_to_sequences(data.question1.values.astype(str))\n",
    "x2 = tk.texts_to_sequences(data.question2.values.astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = sequence.pad_sequences(x1, maxlen=max_len)\n",
    "x2 = sequence.pad_sequences(x2, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained Global Vector (GloVe) word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195875 word vectors.\n"
     ]
    }
   ],
   "source": [
    "word_index = tk.word_index\n",
    "\n",
    "ytrain_enc = np_utils.to_categorical(y)\n",
    "\n",
    "embeddings_index = {}\n",
    "EMBEDDING_DIM = 300\n",
    "EMBEDDING_FILE = \"glove.840B.300d.txt\"\n",
    "with open(EMBEDDING_FILE,encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(values) == EMBEDDING_DIM + 1 :\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next  is to create a word embedding matrix for each word in the word index obtained earlier. If a word doesn't have an embedding in GloVe it will be presented with a zero matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95596/95596 [00:00<00:00, 302859.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=64, kernel_size=5, strides=1, padding=\"valid\")`\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=64, kernel_size=5, strides=1, padding=\"valid\")`\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=64, kernel_size=5, strides=1, padding=\"valid\")`\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:64: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=64, kernel_size=5, strides=1, padding=\"valid\")`\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:73: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, dropout=0.2, recurrent_dropout=0.2)`\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:77: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:78: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    }
   ],
   "source": [
    "max_features = 200000\n",
    "filter_length = 5\n",
    "nb_filter = 64\n",
    "pool_length = 4\n",
    "\n",
    "model = Sequential()\n",
    "print('Build model...')\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=40,\n",
    "                     trainable=False))\n",
    "\n",
    "model1.add(TimeDistributed(Dense(300, activation='relu')))\n",
    "model1.add(Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,)))\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=40,\n",
    "                     trainable=False))\n",
    "\n",
    "model2.add(TimeDistributed(Dense(300, activation='relu')))\n",
    "model2.add(Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,)))\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=40,\n",
    "                     trainable=False))\n",
    "model3.add(Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "model3.add(Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1))\n",
    "\n",
    "model3.add(GlobalMaxPooling1D())\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "model3.add(Dense(300))\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(BatchNormalization())\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=40,\n",
    "                     trainable=False))\n",
    "model4.add(Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1))\n",
    "model4.add(Dropout(0.2))\n",
    "\n",
    "model4.add(Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1))\n",
    "\n",
    "model4.add(GlobalMaxPooling1D())\n",
    "model4.add(Dropout(0.2))\n",
    "\n",
    "model4.add(Dense(300))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(BatchNormalization())\n",
    "model5 = Sequential()\n",
    "model5.add(Embedding(len(word_index) + 1, 300, input_length=40, dropout=0.2))\n",
    "model5.add(LSTM(300, dropout_W=0.2, dropout_U=0.2))\n",
    "\n",
    "model6 = Sequential()\n",
    "model6.add(Embedding(len(word_index) + 1, 300, input_length=40, dropout=0.2))\n",
    "model6.add(LSTM(300, dropout_W=0.2, dropout_U=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = Add()([model1.output,model2.output,model3.output,model4.output,model5.output,model6.output])\n",
    "merged_model = BatchNormalization()(merged_model)\n",
    "\n",
    "merged_model = Dense(300, activation=\"relu\")(merged_model)\n",
    "merged_model = Dropout(0.2)(merged_model)\n",
    "merged_model = BatchNormalization()(merged_model)\n",
    "\n",
    "merged_model = Dense(300, activation=\"relu\")(merged_model)\n",
    "merged_model = Dropout(0.2)(merged_model)\n",
    "merged_model = BatchNormalization()(merged_model)\n",
    "\n",
    "merged_model = Dense(300, activation=\"relu\")(merged_model)\n",
    "merged_model = Dropout(0.2)(merged_model)\n",
    "merged_model = BatchNormalization()(merged_model)\n",
    "\n",
    "merged_model = Dense(300, activation=\"relu\")(merged_model)\n",
    "merged_model = Dropout(0.2)(merged_model)\n",
    "merged_model = BatchNormalization()(merged_model)\n",
    "\n",
    "merged_model = Dense(300, activation=\"relu\")(merged_model)\n",
    "merged_model = Dropout(0.2)(merged_model)\n",
    "merged_model = BatchNormalization()(merged_model)\n",
    "\n",
    "merged_model = Dense(1, activation=\"sigmoid\")(merged_model)\n",
    "\n",
    "new_model = Model([model1.input, model2.input,model3.input,model4.input,model5.input,model6.input], merged_model)\n",
    "\n",
    "new_model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights.h5', monitor='val_acc', save_best_only=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 363861 samples, validate on 40429 samples\n",
      "Epoch 1/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.1426 - accuracy: 0.9411 - val_loss: 0.5925 - val_accuracy: 0.8135\n",
      "Epoch 2/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.1293 - accuracy: 0.9473 - val_loss: 0.5963 - val_accuracy: 0.8156\n",
      "Epoch 3/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.1160 - accuracy: 0.9536 - val_loss: 0.6288 - val_accuracy: 0.8180\n",
      "Epoch 4/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.1049 - accuracy: 0.9576 - val_loss: 0.6347 - val_accuracy: 0.8186\n",
      "Epoch 5/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0978 - accuracy: 0.9607 - val_loss: 0.6595 - val_accuracy: 0.8174\n",
      "Epoch 6/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0917 - accuracy: 0.9639 - val_loss: 0.6886 - val_accuracy: 0.8197\n",
      "Epoch 7/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0847 - accuracy: 0.9667 - val_loss: 0.7426 - val_accuracy: 0.8195\n",
      "Epoch 8/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0787 - accuracy: 0.9692 - val_loss: 0.7502 - val_accuracy: 0.8197\n",
      "Epoch 9/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0759 - accuracy: 0.9706 - val_loss: 0.6991 - val_accuracy: 0.8179\n",
      "Epoch 10/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0717 - accuracy: 0.9717 - val_loss: 0.7255 - val_accuracy: 0.8196\n",
      "Epoch 11/200\n",
      "363861/363861 [==============================] - 95s 262us/step - loss: 0.0693 - accuracy: 0.9730 - val_loss: 0.7740 - val_accuracy: 0.8179\n",
      "Epoch 12/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0658 - accuracy: 0.9744 - val_loss: 0.7168 - val_accuracy: 0.8185\n",
      "Epoch 13/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0626 - accuracy: 0.9756 - val_loss: 0.8401 - val_accuracy: 0.8197\n",
      "Epoch 14/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0605 - accuracy: 0.9766 - val_loss: 0.8453 - val_accuracy: 0.8196\n",
      "Epoch 15/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0583 - accuracy: 0.9774 - val_loss: 0.7872 - val_accuracy: 0.8194\n",
      "Epoch 16/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0565 - accuracy: 0.9785 - val_loss: 0.8556 - val_accuracy: 0.8180\n",
      "Epoch 17/200\n",
      "363861/363861 [==============================] - 97s 265us/step - loss: 0.0555 - accuracy: 0.9787 - val_loss: 0.8768 - val_accuracy: 0.8154\n",
      "Epoch 18/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0535 - accuracy: 0.9793 - val_loss: 0.7769 - val_accuracy: 0.8228\n",
      "Epoch 19/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0525 - accuracy: 0.9797 - val_loss: 0.8291 - val_accuracy: 0.8212\n",
      "Epoch 20/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0507 - accuracy: 0.9806 - val_loss: 0.8138 - val_accuracy: 0.8202\n",
      "Epoch 21/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0495 - accuracy: 0.9811 - val_loss: 0.7753 - val_accuracy: 0.8196\n",
      "Epoch 22/200\n",
      "363861/363861 [==============================] - 97s 265us/step - loss: 0.0491 - accuracy: 0.9816 - val_loss: 0.8104 - val_accuracy: 0.8219\n",
      "Epoch 23/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.0476 - accuracy: 0.9820 - val_loss: 0.7999 - val_accuracy: 0.8226\n",
      "Epoch 24/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0467 - accuracy: 0.9824 - val_loss: 0.9314 - val_accuracy: 0.8199\n",
      "Epoch 25/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0459 - accuracy: 0.9826 - val_loss: 0.8819 - val_accuracy: 0.8204\n",
      "Epoch 26/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0443 - accuracy: 0.9833 - val_loss: 0.8478 - val_accuracy: 0.8197\n",
      "Epoch 27/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0432 - accuracy: 0.9836 - val_loss: 0.8832 - val_accuracy: 0.8218\n",
      "Epoch 28/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0435 - accuracy: 0.9837 - val_loss: 0.7925 - val_accuracy: 0.8220\n",
      "Epoch 29/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0426 - accuracy: 0.9838 - val_loss: 0.8212 - val_accuracy: 0.8199\n",
      "Epoch 30/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0418 - accuracy: 0.9844 - val_loss: 0.8069 - val_accuracy: 0.8240\n",
      "Epoch 31/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0413 - accuracy: 0.9845 - val_loss: 0.7372 - val_accuracy: 0.8220\n",
      "Epoch 32/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0408 - accuracy: 0.9848 - val_loss: 0.8893 - val_accuracy: 0.8209\n",
      "Epoch 33/200\n",
      "363861/363861 [==============================] - 95s 262us/step - loss: 0.0402 - accuracy: 0.9850 - val_loss: 0.8255 - val_accuracy: 0.8220\n",
      "Epoch 34/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0397 - accuracy: 0.9851 - val_loss: 0.8648 - val_accuracy: 0.8233\n",
      "Epoch 35/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0387 - accuracy: 0.9855 - val_loss: 0.8917 - val_accuracy: 0.8231\n",
      "Epoch 36/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0385 - accuracy: 0.9859 - val_loss: 0.9166 - val_accuracy: 0.8167\n",
      "Epoch 37/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0381 - accuracy: 0.9858 - val_loss: 0.8284 - val_accuracy: 0.8234\n",
      "Epoch 38/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0372 - accuracy: 0.9860 - val_loss: 0.9466 - val_accuracy: 0.8214\n",
      "Epoch 39/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0366 - accuracy: 0.9864 - val_loss: 0.9196 - val_accuracy: 0.8208\n",
      "Epoch 40/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0362 - accuracy: 0.9864 - val_loss: 0.8998 - val_accuracy: 0.8220\n",
      "Epoch 41/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0359 - accuracy: 0.9868 - val_loss: 0.8619 - val_accuracy: 0.8240\n",
      "Epoch 42/200\n",
      "363861/363861 [==============================] - 95s 262us/step - loss: 0.0356 - accuracy: 0.9866 - val_loss: 0.8890 - val_accuracy: 0.8196\n",
      "Epoch 43/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0348 - accuracy: 0.9872 - val_loss: 0.8661 - val_accuracy: 0.8209\n",
      "Epoch 44/200\n",
      "363861/363861 [==============================] - 95s 262us/step - loss: 0.0345 - accuracy: 0.9873 - val_loss: 0.9015 - val_accuracy: 0.8200\n",
      "Epoch 45/200\n",
      "363861/363861 [==============================] - 95s 262us/step - loss: 0.0350 - accuracy: 0.9870 - val_loss: 0.8350 - val_accuracy: 0.8204\n",
      "Epoch 46/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0342 - accuracy: 0.9875 - val_loss: 0.9781 - val_accuracy: 0.8197\n",
      "Epoch 47/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0333 - accuracy: 0.9876 - val_loss: 0.9446 - val_accuracy: 0.8187\n",
      "Epoch 48/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0326 - accuracy: 0.9880 - val_loss: 0.8637 - val_accuracy: 0.8227\n",
      "Epoch 49/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0328 - accuracy: 0.9879 - val_loss: 0.9552 - val_accuracy: 0.8219\n",
      "Epoch 50/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0326 - accuracy: 0.9879 - val_loss: 0.9413 - val_accuracy: 0.8211\n",
      "Epoch 51/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0321 - accuracy: 0.9882 - val_loss: 0.8702 - val_accuracy: 0.8235\n",
      "Epoch 52/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0324 - accuracy: 0.9878 - val_loss: 0.8953 - val_accuracy: 0.8223\n",
      "Epoch 53/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0314 - accuracy: 0.9883 - val_loss: 0.9491 - val_accuracy: 0.8225\n",
      "Epoch 55/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0328 - accuracy: 0.9880 - val_loss: 0.8851 - val_accuracy: 0.8174\n",
      "Epoch 56/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0324 - accuracy: 0.9881 - val_loss: 0.8395 - val_accuracy: 0.8248\n",
      "Epoch 57/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0312 - accuracy: 0.9885 - val_loss: 0.9264 - val_accuracy: 0.8227\n",
      "Epoch 58/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0311 - accuracy: 0.9889 - val_loss: 0.9525 - val_accuracy: 0.8232\n",
      "Epoch 59/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.0303 - accuracy: 0.9888 - val_loss: 0.9187 - val_accuracy: 0.8189\n",
      "Epoch 60/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0303 - accuracy: 0.9889 - val_loss: 0.8704 - val_accuracy: 0.8226\n",
      "Epoch 61/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0297 - accuracy: 0.9892 - val_loss: 1.0381 - val_accuracy: 0.8204\n",
      "Epoch 62/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0291 - accuracy: 0.9893 - val_loss: 0.9934 - val_accuracy: 0.8194\n",
      "Epoch 63/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0286 - accuracy: 0.9895 - val_loss: 0.8925 - val_accuracy: 0.8249\n",
      "Epoch 64/200\n",
      "363861/363861 [==============================] - 97s 267us/step - loss: 0.0283 - accuracy: 0.9897 - val_loss: 0.9774 - val_accuracy: 0.8238\n",
      "Epoch 65/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0292 - accuracy: 0.9894 - val_loss: 0.8749 - val_accuracy: 0.8223\n",
      "Epoch 66/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0289 - accuracy: 0.9892 - val_loss: 0.9143 - val_accuracy: 0.8237\n",
      "Epoch 67/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0288 - accuracy: 0.9895 - val_loss: 0.9405 - val_accuracy: 0.8232\n",
      "Epoch 68/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0287 - accuracy: 0.9895 - val_loss: 0.8910 - val_accuracy: 0.8251\n",
      "Epoch 69/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0284 - accuracy: 0.9896 - val_loss: 0.8743 - val_accuracy: 0.8246\n",
      "Epoch 70/200\n",
      "363861/363861 [==============================] - 97s 266us/step - loss: 0.0280 - accuracy: 0.9896 - val_loss: 0.9149 - val_accuracy: 0.8272\n",
      "Epoch 71/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0277 - accuracy: 0.9897 - val_loss: 0.9379 - val_accuracy: 0.8256\n",
      "Epoch 72/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0272 - accuracy: 0.9901 - val_loss: 0.8678 - val_accuracy: 0.8253\n",
      "Epoch 73/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0280 - accuracy: 0.9898 - val_loss: 0.8561 - val_accuracy: 0.8250\n",
      "Epoch 74/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0270 - accuracy: 0.9903 - val_loss: 0.9461 - val_accuracy: 0.8235\n",
      "Epoch 75/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0276 - accuracy: 0.9899 - val_loss: 0.9268 - val_accuracy: 0.8255\n",
      "Epoch 76/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0268 - accuracy: 0.9902 - val_loss: 0.9021 - val_accuracy: 0.8264\n",
      "Epoch 77/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0272 - accuracy: 0.9901 - val_loss: 0.8371 - val_accuracy: 0.8239\n",
      "Epoch 78/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0264 - accuracy: 0.9905 - val_loss: 0.9599 - val_accuracy: 0.8243\n",
      "Epoch 79/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0258 - accuracy: 0.9908 - val_loss: 0.8649 - val_accuracy: 0.8253\n",
      "Epoch 80/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0259 - accuracy: 0.9906 - val_loss: 0.9869 - val_accuracy: 0.8237\n",
      "Epoch 81/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0260 - accuracy: 0.9905 - val_loss: 0.9947 - val_accuracy: 0.8248\n",
      "Epoch 82/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.0256 - accuracy: 0.9906 - val_loss: 0.9587 - val_accuracy: 0.8246\n",
      "Epoch 83/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0252 - accuracy: 0.9910 - val_loss: 0.9824 - val_accuracy: 0.8224\n",
      "Epoch 84/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0257 - accuracy: 0.9906 - val_loss: 0.9108 - val_accuracy: 0.8240\n",
      "Epoch 85/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0255 - accuracy: 0.9908 - val_loss: 0.9894 - val_accuracy: 0.8206\n",
      "Epoch 86/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0258 - accuracy: 0.9906 - val_loss: 0.9274 - val_accuracy: 0.8235\n",
      "Epoch 87/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0246 - accuracy: 0.9911 - val_loss: 0.9260 - val_accuracy: 0.8273\n",
      "Epoch 88/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0250 - accuracy: 0.9910 - val_loss: 0.8842 - val_accuracy: 0.8230\n",
      "Epoch 89/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0251 - accuracy: 0.9908 - val_loss: 0.8824 - val_accuracy: 0.8259\n",
      "Epoch 90/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0248 - accuracy: 0.9910 - val_loss: 0.9508 - val_accuracy: 0.8210\n",
      "Epoch 91/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0253 - accuracy: 0.9907 - val_loss: 0.9497 - val_accuracy: 0.8242\n",
      "Epoch 92/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0246 - accuracy: 0.9912 - val_loss: 1.0283 - val_accuracy: 0.8222\n",
      "Epoch 93/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0248 - accuracy: 0.9911 - val_loss: 0.9210 - val_accuracy: 0.8245\n",
      "Epoch 94/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0241 - accuracy: 0.9913 - val_loss: 1.0014 - val_accuracy: 0.8249\n",
      "Epoch 95/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0241 - accuracy: 0.9911 - val_loss: 0.9296 - val_accuracy: 0.8261\n",
      "Epoch 96/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0243 - accuracy: 0.9911 - val_loss: 0.9047 - val_accuracy: 0.8252\n",
      "Epoch 97/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0242 - accuracy: 0.9914 - val_loss: 0.8699 - val_accuracy: 0.8248\n",
      "Epoch 98/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0246 - accuracy: 0.9911 - val_loss: 1.0001 - val_accuracy: 0.8212\n",
      "Epoch 99/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0235 - accuracy: 0.9914 - val_loss: 0.9259 - val_accuracy: 0.8242\n",
      "Epoch 100/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0236 - accuracy: 0.9916 - val_loss: 0.9705 - val_accuracy: 0.8248\n",
      "Epoch 101/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0242 - accuracy: 0.9912 - val_loss: 0.9879 - val_accuracy: 0.8239\n",
      "Epoch 102/200\n",
      "363861/363861 [==============================] - 95s 262us/step - loss: 0.0243 - accuracy: 0.9912 - val_loss: 0.9480 - val_accuracy: 0.8198\n",
      "Epoch 103/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0228 - accuracy: 0.9916 - val_loss: 1.0224 - val_accuracy: 0.8238\n",
      "Epoch 104/200\n",
      "363861/363861 [==============================] - 97s 265us/step - loss: 0.0239 - accuracy: 0.9913 - val_loss: 1.0657 - val_accuracy: 0.8215\n",
      "Epoch 105/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0240 - accuracy: 0.9913 - val_loss: 0.9385 - val_accuracy: 0.8263\n",
      "Epoch 106/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0237 - accuracy: 0.9913 - val_loss: 0.9425 - val_accuracy: 0.8254\n",
      "Epoch 107/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0234 - accuracy: 0.9915 - val_loss: 0.9890 - val_accuracy: 0.8230\n",
      "Epoch 108/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0227 - accuracy: 0.9920 - val_loss: 1.0621 - val_accuracy: 0.8237\n",
      "Epoch 109/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0231 - accuracy: 0.9917 - val_loss: 0.9573 - val_accuracy: 0.8221\n",
      "Epoch 110/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.0230 - accuracy: 0.9917 - val_loss: 0.9628 - val_accuracy: 0.8244\n",
      "Epoch 111/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0226 - accuracy: 0.9919 - val_loss: 0.9587 - val_accuracy: 0.8279\n",
      "Epoch 112/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0233 - accuracy: 0.9915 - val_loss: 1.0874 - val_accuracy: 0.8236\n",
      "Epoch 113/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0229 - accuracy: 0.9916 - val_loss: 0.8993 - val_accuracy: 0.8302\n",
      "Epoch 114/200\n",
      "363861/363861 [==============================] - 97s 265us/step - loss: 0.0227 - accuracy: 0.9919 - val_loss: 0.9711 - val_accuracy: 0.8247\n",
      "Epoch 115/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0220 - accuracy: 0.9922 - val_loss: 0.9334 - val_accuracy: 0.8250\n",
      "Epoch 116/200\n",
      "363861/363861 [==============================] - 97s 266us/step - loss: 0.0230 - accuracy: 0.9916 - val_loss: 0.9115 - val_accuracy: 0.8254\n",
      "Epoch 117/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0217 - accuracy: 0.9921 - val_loss: 0.9327 - val_accuracy: 0.8274\n",
      "Epoch 118/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0223 - accuracy: 0.9920 - val_loss: 0.9264 - val_accuracy: 0.8230\n",
      "Epoch 119/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0224 - accuracy: 0.9919 - val_loss: 1.0503 - val_accuracy: 0.8254\n",
      "Epoch 120/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0219 - accuracy: 0.9920 - val_loss: 0.9322 - val_accuracy: 0.8289\n",
      "Epoch 121/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0219 - accuracy: 0.9921 - val_loss: 0.9300 - val_accuracy: 0.8276\n",
      "Epoch 122/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0216 - accuracy: 0.9922 - val_loss: 0.9531 - val_accuracy: 0.8241\n",
      "Epoch 123/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0220 - accuracy: 0.9922 - val_loss: 0.9589 - val_accuracy: 0.8240\n",
      "Epoch 124/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0219 - accuracy: 0.9920 - val_loss: 0.8735 - val_accuracy: 0.8317\n",
      "Epoch 125/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0219 - accuracy: 0.9922 - val_loss: 0.8932 - val_accuracy: 0.8281\n",
      "Epoch 126/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0217 - accuracy: 0.9922 - val_loss: 1.0131 - val_accuracy: 0.8280\n",
      "Epoch 127/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.0215 - accuracy: 0.9923 - val_loss: 0.9623 - val_accuracy: 0.8270\n",
      "Epoch 128/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0219 - accuracy: 0.9922 - val_loss: 1.0352 - val_accuracy: 0.8272\n",
      "Epoch 129/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0213 - accuracy: 0.9923 - val_loss: 0.8952 - val_accuracy: 0.8243\n",
      "Epoch 130/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.0208 - accuracy: 0.9926 - val_loss: 0.9604 - val_accuracy: 0.8263\n",
      "Epoch 131/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0216 - accuracy: 0.9922 - val_loss: 1.0142 - val_accuracy: 0.8269\n",
      "Epoch 132/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0215 - accuracy: 0.9923 - val_loss: 0.9595 - val_accuracy: 0.8275\n",
      "Epoch 133/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0210 - accuracy: 0.9924 - val_loss: 1.0054 - val_accuracy: 0.8240\n",
      "Epoch 134/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0210 - accuracy: 0.9924 - val_loss: 0.9675 - val_accuracy: 0.8250\n",
      "Epoch 135/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0213 - accuracy: 0.9924 - val_loss: 1.0033 - val_accuracy: 0.8248\n",
      "Epoch 136/200\n",
      "363861/363861 [==============================] - 97s 265us/step - loss: 0.0217 - accuracy: 0.9924 - val_loss: 0.9146 - val_accuracy: 0.8298\n",
      "Epoch 137/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0207 - accuracy: 0.9927 - val_loss: 0.9186 - val_accuracy: 0.8299\n",
      "Epoch 138/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0210 - accuracy: 0.9925 - val_loss: 0.9521 - val_accuracy: 0.8305\n",
      "Epoch 139/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0207 - accuracy: 0.9926 - val_loss: 0.9304 - val_accuracy: 0.8298\n",
      "Epoch 140/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.0205 - accuracy: 0.9927 - val_loss: 0.9445 - val_accuracy: 0.8243\n",
      "Epoch 141/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0213 - accuracy: 0.9925 - val_loss: 0.9120 - val_accuracy: 0.8262\n",
      "Epoch 142/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0205 - accuracy: 0.9927 - val_loss: 0.9600 - val_accuracy: 0.8246\n",
      "Epoch 143/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0208 - accuracy: 0.9926 - val_loss: 0.9722 - val_accuracy: 0.8284\n",
      "Epoch 144/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0201 - accuracy: 0.9927 - val_loss: 0.8678 - val_accuracy: 0.8254\n",
      "Epoch 145/200\n",
      "363861/363861 [==============================] - 97s 267us/step - loss: 0.0206 - accuracy: 0.9925 - val_loss: 0.9860 - val_accuracy: 0.8286\n",
      "Epoch 146/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0209 - accuracy: 0.9925 - val_loss: 0.9150 - val_accuracy: 0.8286\n",
      "Epoch 147/200\n",
      "363861/363861 [==============================] - 97s 266us/step - loss: 0.0202 - accuracy: 0.9927 - val_loss: 1.0043 - val_accuracy: 0.8273\n",
      "Epoch 148/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0205 - accuracy: 0.9926 - val_loss: 0.9927 - val_accuracy: 0.8281\n",
      "Epoch 149/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0204 - accuracy: 0.9927 - val_loss: 0.9153 - val_accuracy: 0.8282\n",
      "Epoch 150/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0200 - accuracy: 0.9931 - val_loss: 0.9555 - val_accuracy: 0.8270\n",
      "Epoch 151/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0200 - accuracy: 0.9927 - val_loss: 0.9686 - val_accuracy: 0.8263\n",
      "Epoch 152/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0202 - accuracy: 0.9927 - val_loss: 0.9036 - val_accuracy: 0.8260\n",
      "Epoch 153/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0197 - accuracy: 0.9931 - val_loss: 1.0473 - val_accuracy: 0.8255\n",
      "Epoch 154/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0203 - accuracy: 0.9927 - val_loss: 1.0155 - val_accuracy: 0.8285\n",
      "Epoch 155/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0202 - accuracy: 0.9929 - val_loss: 0.9660 - val_accuracy: 0.8283\n",
      "Epoch 156/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.0202 - accuracy: 0.9929 - val_loss: 0.9175 - val_accuracy: 0.8275\n",
      "Epoch 157/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0195 - accuracy: 0.9930 - val_loss: 1.0096 - val_accuracy: 0.8257\n",
      "Epoch 158/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0201 - accuracy: 0.9930 - val_loss: 0.9772 - val_accuracy: 0.8262\n",
      "Epoch 159/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0198 - accuracy: 0.9929 - val_loss: 0.9492 - val_accuracy: 0.8283\n",
      "Epoch 160/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0195 - accuracy: 0.9931 - val_loss: 0.9387 - val_accuracy: 0.8267\n",
      "Epoch 161/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0198 - accuracy: 0.9930 - val_loss: 1.0466 - val_accuracy: 0.8229\n",
      "Epoch 162/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0192 - accuracy: 0.9933 - val_loss: 1.0272 - val_accuracy: 0.8265\n",
      "Epoch 163/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0195 - accuracy: 0.9931 - val_loss: 0.9610 - val_accuracy: 0.8294\n",
      "Epoch 164/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0194 - accuracy: 0.9932 - val_loss: 0.9167 - val_accuracy: 0.8276\n",
      "Epoch 165/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0192 - accuracy: 0.9931 - val_loss: 0.9476 - val_accuracy: 0.8312\n",
      "Epoch 166/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.0191 - accuracy: 0.9932 - val_loss: 1.0237 - val_accuracy: 0.8289\n",
      "Epoch 167/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.0195 - accuracy: 0.9929 - val_loss: 0.9437 - val_accuracy: 0.8271\n",
      "Epoch 169/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.0197 - accuracy: 0.9930 - val_loss: 0.9257 - val_accuracy: 0.8294\n",
      "Epoch 170/200\n",
      "363861/363861 [==============================] - 97s 265us/step - loss: 0.0189 - accuracy: 0.9934 - val_loss: 0.9702 - val_accuracy: 0.8296\n",
      "Epoch 171/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0189 - accuracy: 0.9932 - val_loss: 1.0201 - val_accuracy: 0.8283\n",
      "Epoch 172/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0188 - accuracy: 0.9934 - val_loss: 0.9429 - val_accuracy: 0.8285\n",
      "Epoch 173/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0188 - accuracy: 0.9932 - val_loss: 0.9314 - val_accuracy: 0.8316\n",
      "Epoch 174/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0193 - accuracy: 0.9932 - val_loss: 0.9427 - val_accuracy: 0.8270\n",
      "Epoch 175/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0181 - accuracy: 0.9935 - val_loss: 1.0642 - val_accuracy: 0.8282\n",
      "Epoch 176/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0191 - accuracy: 0.9932 - val_loss: 0.9572 - val_accuracy: 0.8290\n",
      "Epoch 177/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0180 - accuracy: 0.9936 - val_loss: 1.0386 - val_accuracy: 0.8223\n",
      "Epoch 178/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0186 - accuracy: 0.9935 - val_loss: 0.9803 - val_accuracy: 0.8244\n",
      "Epoch 179/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.0185 - accuracy: 0.9934 - val_loss: 1.0486 - val_accuracy: 0.8261\n",
      "Epoch 180/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0183 - accuracy: 0.9935 - val_loss: 1.0386 - val_accuracy: 0.8257\n",
      "Epoch 181/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0190 - accuracy: 0.9933 - val_loss: 0.9890 - val_accuracy: 0.8269\n",
      "Epoch 182/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.0187 - accuracy: 0.9933 - val_loss: 0.9504 - val_accuracy: 0.8298\n",
      "Epoch 183/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0187 - accuracy: 0.9934 - val_loss: 0.9640 - val_accuracy: 0.8295\n",
      "Epoch 184/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.0186 - accuracy: 0.9935 - val_loss: 1.0680 - val_accuracy: 0.8280\n",
      "Epoch 185/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0183 - accuracy: 0.9935 - val_loss: 0.9936 - val_accuracy: 0.8279\n",
      "Epoch 186/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0182 - accuracy: 0.9937 - val_loss: 1.0310 - val_accuracy: 0.8273\n",
      "Epoch 187/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0187 - accuracy: 0.9935 - val_loss: 0.9629 - val_accuracy: 0.8263\n",
      "Epoch 188/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0186 - accuracy: 0.9935 - val_loss: 1.0423 - val_accuracy: 0.8297\n",
      "Epoch 189/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0180 - accuracy: 0.9937 - val_loss: 0.9797 - val_accuracy: 0.8302\n",
      "Epoch 190/200\n",
      "363861/363861 [==============================] - 96s 265us/step - loss: 0.0179 - accuracy: 0.9936 - val_loss: 0.9800 - val_accuracy: 0.8296\n",
      "Epoch 191/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0183 - accuracy: 0.9935 - val_loss: 0.9106 - val_accuracy: 0.8292\n",
      "Epoch 192/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0183 - accuracy: 0.9935 - val_loss: 0.9865 - val_accuracy: 0.8288\n",
      "Epoch 193/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0186 - accuracy: 0.9935 - val_loss: 0.9220 - val_accuracy: 0.8282\n",
      "Epoch 194/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0184 - accuracy: 0.9936 - val_loss: 0.9474 - val_accuracy: 0.8305\n",
      "Epoch 195/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0184 - accuracy: 0.9935 - val_loss: 0.9267 - val_accuracy: 0.8306\n",
      "Epoch 196/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0182 - accuracy: 0.9935 - val_loss: 0.9687 - val_accuracy: 0.8277\n",
      "Epoch 197/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0178 - accuracy: 0.9936 - val_loss: 0.9835 - val_accuracy: 0.8264\n",
      "Epoch 198/200\n",
      "363861/363861 [==============================] - 96s 263us/step - loss: 0.0180 - accuracy: 0.9936 - val_loss: 0.9690 - val_accuracy: 0.8301\n",
      "Epoch 199/200\n",
      "363861/363861 [==============================] - 96s 264us/step - loss: 0.0178 - accuracy: 0.9936 - val_loss: 1.0225 - val_accuracy: 0.8298\n",
      "Epoch 200/200\n",
      "363861/363861 [==============================] - 97s 265us/step - loss: 0.0177 - accuracy: 0.9936 - val_loss: 0.9995 - val_accuracy: 0.8272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7efe28188908>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.fit([x1, x2, x1, x2, x1, x2], y=y, batch_size=384, nb_epoch=200,\n",
    "                 verbose=1, validation_split=0.1, shuffle=True, callbacks=[checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
